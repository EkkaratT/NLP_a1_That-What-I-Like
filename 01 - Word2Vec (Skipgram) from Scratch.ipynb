{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word2Vec\n",
        "\n",
        "Let's work on skipgram-based implementation of word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\Ekkar\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "corpus = brown.sents(categories='news')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get word sequences and unique words\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "vocab = list(set(flatten(corpus)))\n",
        "# vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "#numericalization\n",
        "word2index = {w: i for i, w in enumerate(vocab)}\n",
        "# print(word2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14394\n"
          ]
        }
      ],
      "source": [
        "#vocab size\n",
        "voc_size = len(vocab)\n",
        "print(voc_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#append UNK\n",
        "vocab.append('<UNK>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2index['<UNK>'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#just in case we need to use\n",
        "index2word = {v:k for k, v in word2index.items()} "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for c in corpus:\n",
        "#     print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_batch(batch_size, word_sequence):\n",
        "    \n",
        "    window_size = 2\n",
        "    # Make skip gram of one size window\n",
        "    skip_grams = []\n",
        "    # loop each word sequence\n",
        "    # we starts from 1 because 0 has no context\n",
        "    # we stop at second last for the same reason\n",
        "    for sent in corpus:\n",
        "        for i in range(window_size, len(sent) - window_size):\n",
        "            target = word2index[sent[i]]\n",
        "\n",
        "            context = []\n",
        "            for j in range(1, window_size):\n",
        "                context.append(word2index[sent[i - j]])\n",
        "                context.append(word2index[sent[i + j]])\n",
        "\n",
        "            # for each outside word, append to a skip_grams\n",
        "            for w in context:\n",
        "                skip_grams.append([target, w])\n",
        "    \n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
        "        \n",
        "    for i in random_index:\n",
        "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
        "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
        "            \n",
        "    return np.array(random_inputs), np.array(random_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  [[14327]\n",
            " [ 3389]]\n",
            "Target:  [[ 8079]\n",
            " [14187]]\n"
          ]
        }
      ],
      "source": [
        "#testing the method\n",
        "batch_size = 2 # mini-batch size\n",
        "input_batch, target_batch = random_batch(batch_size, corpus)\n",
        "\n",
        "print(\"Input: \", input_batch)\n",
        "print(\"Target: \", target_batch)\n",
        "\n",
        "#we will convert them to tensor during training, so don't worry..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
        "\n",
        "where $P(w_{t+j} | w_t; \\theta) = $\n",
        "\n",
        "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
        "\n",
        "where $o$ is the outside words and $c$ is the center word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Skipgram(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, emb_size, word2index):\n",
        "        super(Skipgram,self).__init__()\n",
        "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
        "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
        "        self.word2index  = word2index\n",
        "    \n",
        "    def forward(self, center_words, target_words, all_vocabs):\n",
        "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
        "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
        "        all_embeds    = self.embedding_u(all_vocabs) #   [batch_size, voc_size, emb_size]\n",
        "        \n",
        "        scores      = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
        "\n",
        "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        #[batch_size, voc_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, voc_size, 1] = [batch_size, voc_size]\n",
        "\n",
        "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
        "        # scalar (loss must be scalar)    \n",
        "            \n",
        "        return nll # negative log likelihood\n",
        "    \n",
        "    def get_embed(self, word):\n",
        "        word2index = self.word2index\n",
        "        \n",
        "        try:\n",
        "            index = word2index[word]\n",
        "        except:\n",
        "            index = word2index['<UNK>']\n",
        "            \n",
        "        word = torch.LongTensor([index])\n",
        "        \n",
        "        embed_c = self.embedding_v(word)\n",
        "        embed_o = self.embedding_u(word)\n",
        "        embed   = (embed_c + embed_o) / 2\n",
        "        \n",
        "        return embed[0][0].item(), embed[0][1].item()\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size     = 2 # mini-batch size\n",
        "embedding_size = 2 #so we can later plot\n",
        "model          = Skipgram(voc_size, embedding_size, word2index)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 14395])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
        "    return torch.LongTensor(idxs)\n",
        "\n",
        "#use for the normalized term in the probability calculation\n",
        "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab))  # [batch_size, voc_size]\n",
        "all_vocabs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | cost: 10.656777 | time: 0m 4.000000s\n",
            "Epoch: 20 | cost: 12.350922 | time: 0m 8.000000s\n",
            "Epoch: 30 | cost: 11.702185 | time: 0m 13.000000s\n",
            "Epoch: 40 | cost: 10.465929 | time: 0m 17.000000s\n",
            "Epoch: 50 | cost: 9.686190 | time: 0m 21.000000s\n",
            "Epoch: 60 | cost: 10.120502 | time: 0m 26.000000s\n",
            "Epoch: 70 | cost: 10.984463 | time: 0m 30.000000s\n",
            "Epoch: 80 | cost: 11.165031 | time: 0m 34.000000s\n",
            "Epoch: 90 | cost: 9.309835 | time: 0m 39.000000s\n",
            "Epoch: 100 | cost: 10.027914 | time: 0m 43.000000s\n",
            "\n",
            "Complete: \n",
            "Total Loss: 10.03 | Time Taken: 0 minutes and 43 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Training\n",
        "num_epochs = 100\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    #get batch\n",
        "    input_batch, target_batch = random_batch(batch_size, corpus)\n",
        "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
        "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
        "\n",
        "    #predict\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(input_batch, target_batch, all_vocabs)\n",
        "    \n",
        "    #backprogate\n",
        "    loss.backward()\n",
        "\n",
        "    #update alpha\n",
        "    optimizer.step()\n",
        "    \n",
        "    end = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
        "\n",
        "    #print the loss\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs:.6f}s\")\n",
        "\n",
        "print(f\"\\nComplete: \\nTotal Loss: {loss:2.2f} | Time Taken: {epoch_mins} minutes and {epoch_secs} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def open_file(path_to_file):\n",
        "    content = []  # Initialize content to an empty list to avoid returning None\n",
        "    try:\n",
        "        with open(path_to_file, 'r') as file:\n",
        "            content = file.readlines()  # Read all lines of the file into a list\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file {path_to_file} does not exist.\")  # File not found error\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")  # Handle any other exceptions (e.g., permission issues)\n",
        "\n",
        "    return content  # Return content even if it's empty, but not None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"file/word-test.v1.1.txt\"\n",
        "\n",
        "content = open_file(file_path)\n",
        "\n",
        "semantic = []\n",
        "syntatic = []\n",
        "\n",
        "current_test = semantic\n",
        "for sent in content:\n",
        "    if sent[0] == ':':\n",
        "        current_test = syntatic\n",
        "        continue\n",
        "    \n",
        "    current_test.append(sent.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_space = []\n",
        "\n",
        "for word in vocab:\n",
        "    vector_space.append(model.get_embed(word))\n",
        "\n",
        "vector_space = np.array(vector_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#scipy version\n",
        "from scipy import spatial\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    norm_a = a / np.linalg.norm(a)  # Normalize vector a\n",
        "    norm_b = b / np.linalg.norm(b)  # Normalize vector b\n",
        "    return 1 - spatial.distance.cosine(norm_a, norm_b)  # Cosine similarity after normalization\n",
        "\n",
        "\n",
        "def cos_sim_scores(vector_space, target_vector):\n",
        "    scores = []\n",
        "    for each_vect in vector_space:\n",
        "        scores.append(cos_sim(target_vector, each_vect))\n",
        "\n",
        "    return np.array(scores)\n",
        "\n",
        "def similarity(model, test_data):\n",
        "    words = test_data.split(\" \")\n",
        "    embeddings = [np.array(model.get_embed(word)) for word in words[:3]]  # Precompute embeddings for all words\n",
        "    embed0, embed1, embed2 = embeddings  # Unpack embeddings\n",
        "    similar_vector = embed1 - embed0 + embed2  # Perform vector arithmetic\n",
        "\n",
        "    similarity_scores = cos_sim_scores(vector_space, similar_vector)\n",
        "    max_score_idx = np.argmax(similarity_scores)\n",
        "    similar_word = index2word[max_score_idx]\n",
        "\n",
        "    return similar_word == words[3]  # Directly return the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Semantic accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "sem_total = len(semantic)\n",
        "sem_correct = 0\n",
        "for sent in semantic:\n",
        "    if similarity(model, sent):\n",
        "        sem_correct += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic accuracy: 0.00\n"
          ]
        }
      ],
      "source": [
        "sem_accuracy = sem_correct / sem_total\n",
        "print(f\"Semantic accuracy: {sem_accuracy:2.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Syntactic Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "syn_total = len(syntatic)\n",
        "syn_correct = 0\n",
        "for sent in syntatic:\n",
        "    if similarity(model, sent):\n",
        "        syn_correct += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Syntatic accuracy: 0.00\n"
          ]
        }
      ],
      "source": [
        "syn_accuracy = syn_correct / syn_total\n",
        "print(f\"Syntatic accuracy: {syn_accuracy:2.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarity Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"file/wordsim_similarity_goldstandard.txt\"\n",
        "\n",
        "content = open_file(file_path)\n",
        "\n",
        "sim_data = []\n",
        "\n",
        "for sent in content:\n",
        "    sim_data.append(sent.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_similarity(model, test_data):\n",
        "    words = test_data.split(\"\\t\")\n",
        "\n",
        "    embed0 = np.array(model.get_embed(words[0].strip()))\n",
        "    embed1 = np.array(model.get_embed(words[1].strip()))\n",
        "\n",
        "    similarity_model = embed1 @ embed0.T\n",
        "    similarity_provided = float(words[2].strip())\n",
        "\n",
        "    return similarity_provided, similarity_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_scores = []\n",
        "model_scores = []\n",
        "for sent in sim_data:\n",
        "    ds_score, model_score = compute_similarity(model, sent)\n",
        "\n",
        "    ds_scores.append(ds_score)\n",
        "    model_scores.append(model_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation between the dataset similarity metrics and models’ dot product is -0.07.\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "correlation = spearmanr(ds_scores, model_scores)[0]\n",
        "\n",
        "print(f\"Correlation between the dataset similarity metrics and models’ dot product is {correlation:2.2f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and arguments saved to model\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Define the folder where want to save the files\n",
        "model_folder = 'model'  # Change this to your desired folder path\n",
        "\n",
        "# Save the model's state_dict\n",
        "torch.save(model.state_dict(), f'{model_folder}/skipgram.model')\n",
        "\n",
        "# Save the arguments (such as voc_size, emb_size, word2index)\n",
        "skipgram_args = {\n",
        "    'vocab_size': voc_size,\n",
        "    'emb_size': embedding_size,\n",
        "    'word2index': word2index,\n",
        "}\n",
        "with open(f'{model_folder}/skipgram.args', 'wb') as f:\n",
        "    pickle.dump(skipgram_args, f)\n",
        "\n",
        "print(f\"Model and arguments saved to {model_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word2index.pkl saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Save the word2index dictionary as a pickle file\n",
        "with open('model/word2index.pkl', 'wb') as f:\n",
        "    pickle.dump(word2index, f)\n",
        "\n",
        "print(\"word2index.pkl saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ekkar\\AppData\\Local\\Temp\\ipykernel_6284\\3156277920.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_skipgram.load_state_dict(torch.load(f'{model_folder}/skipgram.model'))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Define the folder where the files are saved\n",
        "model_folder = 'model'  # Change this to the folder where you saved the files\n",
        "\n",
        "# Load the arguments from the pickle file\n",
        "with open(f'{model_folder}/skipgram.args', 'rb') as f:\n",
        "    skipgram_args = pickle.load(f)\n",
        "\n",
        "# Define the model class and initialize it with the loaded arguments\n",
        "# Make sure the model class and arguments match the training code\n",
        "model_skipgram = Skipgram(**skipgram_args)  # Assuming you have a skipgram model class\n",
        "\n",
        "# Now, load the model weights (this should be from skipgram.model, not skipgram.args)\n",
        "model_skipgram.load_state_dict(torch.load(f'{model_folder}/skipgram.model'))\n",
        "\n",
        "# Now the model is loaded with the arguments and weights, and you're ready to use it\n",
        "model_skipgram.eval()  # Set the model to evaluation mode if you're not training\n",
        "\n",
        "print(\"Model loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-0.09687146544456482, -0.5613371133804321)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_skipgram.get_embed('the')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
