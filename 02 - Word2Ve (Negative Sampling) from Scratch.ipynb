{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word2Vec (Negative Sampling)\n",
        "\n",
        "Let's work on negative-sampling based implementation of word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('1.21.5', '2.5.1+cu118')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.__version__, torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\Ekkar\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "corpus = brown.sents(categories='news')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tulsa',\n",
              " 'soldiers',\n",
              " 'Steve',\n",
              " 'adapting',\n",
              " 'event',\n",
              " 'oases',\n",
              " 'conspicuously',\n",
              " 'drill',\n",
              " 'enlivened',\n",
              " 'junta']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#get word sequences and unique words\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "vocab = list(set(flatten(corpus)))\n",
        "vocab[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "#numericalization\n",
        "word2index = {w: i for i, w in enumerate(vocab)}\n",
        "# print(word2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14394\n"
          ]
        }
      ],
      "source": [
        "#vocab size\n",
        "voc_size = len(vocab)\n",
        "print(voc_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#append UNK\n",
        "vocab.append('<UNK>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2index['<UNK>'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#just in case we need to use\n",
        "index2word = {v:k for k, v in word2index.items()} "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for c in corpus:\n",
        "#     print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_batch(batch_size, word_sequence):\n",
        "    \n",
        "    window_size = 2\n",
        "    # Make skip gram of one size window\n",
        "    skip_grams = []\n",
        "    # loop each word sequence\n",
        "    # we starts from 1 because 0 has no context\n",
        "    # we stop at second last for the same reason\n",
        "    for sent in corpus:\n",
        "        for i in range(window_size, len(sent) - window_size):\n",
        "            target = word2index[sent[i]]\n",
        "\n",
        "            context = []\n",
        "            for j in range(1, window_size):\n",
        "                context.append(word2index[sent[i - j]])\n",
        "                context.append(word2index[sent[i + j]])\n",
        "\n",
        "            # for each outside word, append to a skip_grams\n",
        "            for w in context:\n",
        "                skip_grams.append([target, w])\n",
        "    \n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
        "        \n",
        "    for i in random_index:\n",
        "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
        "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
        "            \n",
        "    return np.array(random_inputs), np.array(random_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  [[ 1374]\n",
            " [12330]]\n",
            "Target:  [[1022]\n",
            " [9720]]\n"
          ]
        }
      ],
      "source": [
        "#testing the method\n",
        "batch_size = 2 # mini-batch size\n",
        "input_batch, target_batch = random_batch(batch_size, corpus)\n",
        "\n",
        "print(\"Input: \",  input_batch)\n",
        "print(\"Target: \", target_batch)\n",
        "\n",
        "#we will convert them to tensor during training, so don't worry..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((2, 1), (2, 1))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch.shape, target_batch.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Negative Sampling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram distribution\n",
        "\n",
        "$$P(w)=U(w)^{3/4}/Z$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "Z = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_count = Counter(flatten(corpus))\n",
        "num_total_words = sum([c for w, c in word_count.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "481"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_count['as']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100554"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "unigram_table = []\n",
        "\n",
        "for vo in vocab:\n",
        "    unigram_table.extend([vo] * int(((word_count[vo]/num_total_words)**0.75)/Z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'event': 1,\n",
              "         'plus': 1,\n",
              "         'seemed': 1,\n",
              "         'What': 1,\n",
              "         'done': 1,\n",
              "         'led': 1,\n",
              "         'while': 2,\n",
              "         'passed': 1,\n",
              "         'Austin': 1,\n",
              "         'what': 4,\n",
              "         'carry': 1,\n",
              "         'spirit': 1,\n",
              "         'civil': 1,\n",
              "         'we': 4,\n",
              "         \"Kennedy's\": 1,\n",
              "         'battle': 1,\n",
              "         'counties': 1,\n",
              "         'worth': 1,\n",
              "         'Other': 1,\n",
              "         'front': 1,\n",
              "         'private': 1,\n",
              "         'junior': 1,\n",
              "         'billion': 1,\n",
              "         'Sen.': 1,\n",
              "         'condition': 1,\n",
              "         'School': 1,\n",
              "         'vote': 2,\n",
              "         'result': 2,\n",
              "         'term': 1,\n",
              "         'up': 8,\n",
              "         'Texas': 3,\n",
              "         'ball': 2,\n",
              "         'All': 1,\n",
              "         'level': 1,\n",
              "         'farm': 1,\n",
              "         'Congo': 2,\n",
              "         'land': 1,\n",
              "         'pay': 2,\n",
              "         'leading': 1,\n",
              "         'relations': 1,\n",
              "         'bank': 1,\n",
              "         'color': 1,\n",
              "         'collection': 1,\n",
              "         'period': 1,\n",
              "         'call': 1,\n",
              "         'Robert': 2,\n",
              "         'E.': 2,\n",
              "         'Jan.': 1,\n",
              "         'name': 1,\n",
              "         'returned': 1,\n",
              "         'funds': 2,\n",
              "         'reasons': 1,\n",
              "         '20': 1,\n",
              "         'Mayor': 1,\n",
              "         'key': 1,\n",
              "         'After': 1,\n",
              "         'housing': 1,\n",
              "         'they': 9,\n",
              "         'held': 2,\n",
              "         'trial': 1,\n",
              "         'our': 3,\n",
              "         'recent': 1,\n",
              "         'fine': 1,\n",
              "         'charter': 1,\n",
              "         'green': 1,\n",
              "         'He': 9,\n",
              "         'down': 3,\n",
              "         'summer': 1,\n",
              "         'never': 2,\n",
              "         'grants': 1,\n",
              "         'National': 2,\n",
              "         'Mr.': 8,\n",
              "         'become': 1,\n",
              "         'shown': 1,\n",
              "         'negotiations': 1,\n",
              "         'turn': 1,\n",
              "         'unions': 1,\n",
              "         'close': 1,\n",
              "         'Warren': 1,\n",
              "         'above': 1,\n",
              "         'Secretary': 1,\n",
              "         'Thomas': 1,\n",
              "         'District': 1,\n",
              "         'having': 1,\n",
              "         'teaching': 1,\n",
              "         'least': 1,\n",
              "         'if': 4,\n",
              "         'wages': 1,\n",
              "         'must': 3,\n",
              "         'Soviet': 2,\n",
              "         'bus': 1,\n",
              "         'financial': 1,\n",
              "         'Arnold': 1,\n",
              "         'These': 1,\n",
              "         'leader': 1,\n",
              "         'probably': 1,\n",
              "         'receive': 1,\n",
              "         'month': 2,\n",
              "         'Church': 1,\n",
              "         'Cuba': 1,\n",
              "         'Mickey': 1,\n",
              "         'most': 4,\n",
              "         'old': 1,\n",
              "         'Moscow': 1,\n",
              "         'Christian': 1,\n",
              "         'union': 1,\n",
              "         'Eisenhower': 1,\n",
              "         'society': 1,\n",
              "         'Louis': 1,\n",
              "         'Center': 1,\n",
              "         'wife': 2,\n",
              "         'game': 3,\n",
              "         'services': 1,\n",
              "         'to': 55,\n",
              "         'Last': 1,\n",
              "         'Los': 1,\n",
              "         'many': 3,\n",
              "         'foreign': 2,\n",
              "         'North': 2,\n",
              "         'forces': 1,\n",
              "         'heard': 1,\n",
              "         \"it's\": 1,\n",
              "         'If': 2,\n",
              "         'little': 2,\n",
              "         'whole': 1,\n",
              "         'Air': 1,\n",
              "         'however': 2,\n",
              "         'matter': 1,\n",
              "         'industry': 2,\n",
              "         'try': 1,\n",
              "         'say': 1,\n",
              "         'marriage': 1,\n",
              "         'double': 1,\n",
              "         'better': 2,\n",
              "         '&': 2,\n",
              "         'still': 2,\n",
              "         'nearly': 1,\n",
              "         'them': 5,\n",
              "         'job': 1,\n",
              "         'and': 55,\n",
              "         'gin': 1,\n",
              "         'by': 18,\n",
              "         'like': 3,\n",
              "         'international': 1,\n",
              "         'August': 1,\n",
              "         'its': 8,\n",
              "         'Fulton': 1,\n",
              "         'agency': 1,\n",
              "         'problem': 2,\n",
              "         'opportunity': 1,\n",
              "         'Laos': 2,\n",
              "         'Education': 1,\n",
              "         'often': 1,\n",
              "         'particularly': 1,\n",
              "         'victory': 1,\n",
              "         'annual': 2,\n",
              "         'explained': 1,\n",
              "         'D.': 1,\n",
              "         'staff': 2,\n",
              "         'Howard': 1,\n",
              "         'joined': 1,\n",
              "         'near': 1,\n",
              "         'move': 1,\n",
              "         'or': 8,\n",
              "         'well': 2,\n",
              "         'really': 1,\n",
              "         'me': 2,\n",
              "         'General': 1,\n",
              "         'given': 2,\n",
              "         'association': 1,\n",
              "         \"I'm\": 1,\n",
              "         'round': 1,\n",
              "         'continue': 1,\n",
              "         'return': 1,\n",
              "         'full': 1,\n",
              "         'house': 2,\n",
              "         'hard': 1,\n",
              "         'nations': 1,\n",
              "         'They': 3,\n",
              "         'with': 19,\n",
              "         'White': 2,\n",
              "         'fight': 1,\n",
              "         'reports': 1,\n",
              "         'something': 1,\n",
              "         'judge': 1,\n",
              "         ',': 108,\n",
              "         'water': 1,\n",
              "         'anti-trust': 1,\n",
              "         'elected': 1,\n",
              "         'times': 1,\n",
              "         'family': 2,\n",
              "         'rules': 1,\n",
              "         'left': 2,\n",
              "         'boy': 1,\n",
              "         'plane': 1,\n",
              "         'proposed': 1,\n",
              "         'other': 7,\n",
              "         '11': 1,\n",
              "         'urged': 1,\n",
              "         'workers': 1,\n",
              "         'further': 1,\n",
              "         'Both': 1,\n",
              "         'think': 2,\n",
              "         'wide': 1,\n",
              "         'legislation': 1,\n",
              "         'required': 1,\n",
              "         'traffic': 1,\n",
              "         'drive': 1,\n",
              "         'Geneva': 1,\n",
              "         'members': 4,\n",
              "         'amount': 1,\n",
              "         'guests': 1,\n",
              "         'government': 3,\n",
              "         'date': 1,\n",
              "         'Miss': 3,\n",
              "         'off': 4,\n",
              "         'makes': 1,\n",
              "         'plans': 1,\n",
              "         'nuclear': 1,\n",
              "         'present': 2,\n",
              "         'letters': 1,\n",
              "         'which': 10,\n",
              "         'against': 4,\n",
              "         '30': 1,\n",
              "         'ready': 1,\n",
              "         'six': 1,\n",
              "         'talk': 1,\n",
              "         'Frank': 1,\n",
              "         '!': 2,\n",
              "         'Khrushchev': 1,\n",
              "         'Saturday': 2,\n",
              "         'independence': 1,\n",
              "         'told': 3,\n",
              "         'record': 2,\n",
              "         'possible': 2,\n",
              "         'value': 1,\n",
              "         'come': 2,\n",
              "         'court': 2,\n",
              "         'season': 2,\n",
              "         'being': 3,\n",
              "         'war': 1,\n",
              "         'going': 1,\n",
              "         'opinion': 1,\n",
              "         'Aj': 1,\n",
              "         'university': 2,\n",
              "         'served': 1,\n",
              "         'she': 2,\n",
              "         'how': 2,\n",
              "         ')': 8,\n",
              "         'saw': 1,\n",
              "         '200': 1,\n",
              "         'at': 21,\n",
              "         'Senator': 1,\n",
              "         'some': 5,\n",
              "         'employees': 1,\n",
              "         'several': 2,\n",
              "         'labor': 1,\n",
              "         'Martin': 1,\n",
              "         'Friday': 2,\n",
              "         'driving': 1,\n",
              "         'election': 2,\n",
              "         'position': 1,\n",
              "         'League': 1,\n",
              "         'Republican': 2,\n",
              "         'William': 2,\n",
              "         'series': 1,\n",
              "         'entire': 1,\n",
              "         'persons': 1,\n",
              "         'town': 1,\n",
              "         'art': 1,\n",
              "         'every': 1,\n",
              "         'under': 4,\n",
              "         'P.': 1,\n",
              "         'Judge': 2,\n",
              "         'stand': 1,\n",
              "         'able': 1,\n",
              "         'Avenue': 1,\n",
              "         'City': 3,\n",
              "         'see': 2,\n",
              "         'W.': 2,\n",
              "         'Then': 1,\n",
              "         'go': 2,\n",
              "         'St.': 2,\n",
              "         'spent': 1,\n",
              "         'is': 24,\n",
              "         'received': 2,\n",
              "         'International': 1,\n",
              "         'San': 1,\n",
              "         'coach': 1,\n",
              "         'children': 3,\n",
              "         'House': 4,\n",
              "         'first': 7,\n",
              "         'bills': 1,\n",
              "         'same': 2,\n",
              "         'Now': 1,\n",
              "         'year': 7,\n",
              "         'additional': 1,\n",
              "         'according': 1,\n",
              "         'national': 2,\n",
              "         'would': 10,\n",
              "         'G.': 1,\n",
              "         'had': 12,\n",
              "         'project': 2,\n",
              "         'run': 2,\n",
              "         'Thompson': 1,\n",
              "         'recently': 1,\n",
              "         '10': 2,\n",
              "         'basis': 1,\n",
              "         'Service': 1,\n",
              "         'presented': 1,\n",
              "         'especially': 1,\n",
              "         'calls': 1,\n",
              "         'might': 2,\n",
              "         'immediate': 1,\n",
              "         'prevent': 1,\n",
              "         'Williams': 1,\n",
              "         'meet': 2,\n",
              "         'textile': 1,\n",
              "         'party': 2,\n",
              "         'point': 1,\n",
              "         'property': 1,\n",
              "         'organization': 1,\n",
              "         'Feb.': 1,\n",
              "         'Dr.': 2,\n",
              "         'Jones': 1,\n",
              "         'finished': 1,\n",
              "         'word': 1,\n",
              "         '100': 1,\n",
              "         'honor': 1,\n",
              "         'club': 2,\n",
              "         'reported': 2,\n",
              "         'director': 2,\n",
              "         'my': 2,\n",
              "         'getting': 1,\n",
              "         'following': 1,\n",
              "         'spring': 1,\n",
              "         'development': 1,\n",
              "         'Mrs.': 11,\n",
              "         'thought': 1,\n",
              "         'although': 1,\n",
              "         'pressure': 1,\n",
              "         'New': 5,\n",
              "         'were': 11,\n",
              "         'tomorrow': 1,\n",
              "         '60': 1,\n",
              "         'short': 1,\n",
              "         'Hillsboro': 1,\n",
              "         'fees': 1,\n",
              "         '3': 1,\n",
              "         'Their': 1,\n",
              "         'mother': 1,\n",
              "         'making': 1,\n",
              "         'including': 1,\n",
              "         'strike': 1,\n",
              "         'J.': 2,\n",
              "         'College': 1,\n",
              "         'Wednesday': 1,\n",
              "         'needs': 1,\n",
              "         'has': 12,\n",
              "         'local': 2,\n",
              "         'using': 1,\n",
              "         'play': 2,\n",
              "         'effective': 1,\n",
              "         'cut': 1,\n",
              "         'all': 8,\n",
              "         'ruled': 1,\n",
              "         '18': 1,\n",
              "         'happy': 1,\n",
              "         '(': 8,\n",
              "         'speed': 1,\n",
              "         'aid': 1,\n",
              "         'opening': 1,\n",
              "         'right': 2,\n",
              "         'products': 1,\n",
              "         'increase': 1,\n",
              "         'education': 2,\n",
              "         'serious': 1,\n",
              "         'week': 5,\n",
              "         'shot': 1,\n",
              "         'afternoon': 1,\n",
              "         'conspiracy': 1,\n",
              "         'headquarters': 1,\n",
              "         '9': 1,\n",
              "         'population': 1,\n",
              "         'of': 69,\n",
              "         'conference': 1,\n",
              "         'June': 1,\n",
              "         'evidence': 1,\n",
              "         'James': 2,\n",
              "         'knee': 1,\n",
              "         'your': 1,\n",
              "         'AP': 1,\n",
              "         'not': 11,\n",
              "         'ever': 2,\n",
              "         'about': 7,\n",
              "         'means': 1,\n",
              "         'Stein': 1,\n",
              "         'want': 1,\n",
              "         'playing': 1,\n",
              "         'driven': 1,\n",
              "         'years': 5,\n",
              "         'Francisco': 1,\n",
              "         'chairman': 2,\n",
              "         'eight': 2,\n",
              "         'Hospital': 1,\n",
              "         'We': 2,\n",
              "         'indicated': 1,\n",
              "         'coming': 1,\n",
              "         'back': 4,\n",
              "         'paid': 1,\n",
              "         'Wagner': 1,\n",
              "         'officers': 1,\n",
              "         'bond': 1,\n",
              "         'On': 2,\n",
              "         'college': 1,\n",
              "         'C.': 2,\n",
              "         'before': 5,\n",
              "         'wanted': 1,\n",
              "         'among': 1,\n",
              "         'along': 2,\n",
              "         'secrets': 1,\n",
              "         'Thursday': 1,\n",
              "         'Among': 1,\n",
              "         '1960': 2,\n",
              "         'found': 2,\n",
              "         'games': 1,\n",
              "         'libraries': 1,\n",
              "         'cost': 1,\n",
              "         'Congress': 1,\n",
              "         'yards': 1,\n",
              "         'Department': 1,\n",
              "         'county': 2,\n",
              "         'seem': 1,\n",
              "         'very': 2,\n",
              "         'luncheon': 1,\n",
              "         'offered': 1,\n",
              "         'effort': 1,\n",
              "         'More': 1,\n",
              "         'you': 3,\n",
              "         'stock': 1,\n",
              "         'Portland': 1,\n",
              "         'students': 1,\n",
              "         'then': 3,\n",
              "         'Co.': 2,\n",
              "         \"It's\": 1,\n",
              "         'firms': 1,\n",
              "         'dollars': 1,\n",
              "         'Jim': 1,\n",
              "         'cars': 1,\n",
              "         'Oct.': 1,\n",
              "         '22': 1,\n",
              "         'car': 3,\n",
              "         'secretary': 1,\n",
              "         'her': 5,\n",
              "         \"doesn't\": 1,\n",
              "         'U.': 1,\n",
              "         'West': 1,\n",
              "         ':': 7,\n",
              "         'face': 1,\n",
              "         'hands': 1,\n",
              "         'dropped': 1,\n",
              "         'Smith': 1,\n",
              "         'brought': 1,\n",
              "         'kind': 1,\n",
              "         'said': 15,\n",
              "         'country': 1,\n",
              "         'Orleans': 1,\n",
              "         'headed': 1,\n",
              "         'committee': 2,\n",
              "         'knew': 1,\n",
              "         'Joseph': 1,\n",
              "         'called': 2,\n",
              "         'early': 2,\n",
              "         'home': 6,\n",
              "         'R.': 1,\n",
              "         'daughter': 2,\n",
              "         'good': 3,\n",
              "         'demand': 1,\n",
              "         \"don't\": 1,\n",
              "         'television': 1,\n",
              "         'should': 3,\n",
              "         'president': 3,\n",
              "         'Old': 1,\n",
              "         'security': 1,\n",
              "         'road': 1,\n",
              "         'Player': 2,\n",
              "         'market': 2,\n",
              "         'question': 1,\n",
              "         'resolution': 1,\n",
              "         'added': 2,\n",
              "         'floor': 1,\n",
              "         'David': 1,\n",
              "         'And': 2,\n",
              "         'dinner': 1,\n",
              "         'was': 24,\n",
              "         'took': 3,\n",
              "         'went': 2,\n",
              "         'so': 4,\n",
              "         '8': 1,\n",
              "         'Houston': 1,\n",
              "         'program': 4,\n",
              "         'already': 1,\n",
              "         'form': 1,\n",
              "         'Association': 1,\n",
              "         'Richard': 2,\n",
              "         'Mitchell': 1,\n",
              "         'says': 2,\n",
              "         'Street': 1,\n",
              "         'attend': 1,\n",
              "         'measure': 1,\n",
              "         'half': 1,\n",
              "         'believe': 1,\n",
              "         'Bob': 1,\n",
              "         'British': 1,\n",
              "         'faculty': 1,\n",
              "         'food': 1,\n",
              "         'primary': 1,\n",
              "         'best': 2,\n",
              "         'system': 3,\n",
              "         'Central': 1,\n",
              "         'bargaining': 1,\n",
              "         'cent': 3,\n",
              "         'areas': 1,\n",
              "         'without': 3,\n",
              "         'South': 1,\n",
              "         'when': 6,\n",
              "         'chief': 1,\n",
              "         'last': 8,\n",
              "         'research': 1,\n",
              "         'N.': 1,\n",
              "         'award': 1,\n",
              "         'Republicans': 1,\n",
              "         'Hughes': 1,\n",
              "         'third': 2,\n",
              "         'For': 2,\n",
              "         'turned': 1,\n",
              "         '15': 1,\n",
              "         'out': 8,\n",
              "         'community': 1,\n",
              "         'health': 1,\n",
              "         '25': 1,\n",
              "         'two': 7,\n",
              "         'district': 1,\n",
              "         'Bill': 1,\n",
              "         'part': 2,\n",
              "         'city': 3,\n",
              "         'son': 1,\n",
              "         'officials': 1,\n",
              "         'effect': 1,\n",
              "         'similar': 1,\n",
              "         'pass': 1,\n",
              "         'as': 18,\n",
              "         'into': 6,\n",
              "         'worked': 1,\n",
              "         'Some': 1,\n",
              "         'Ohio': 1,\n",
              "         'hit': 2,\n",
              "         'large': 1,\n",
              "         'room': 1,\n",
              "         'far': 2,\n",
              "         'Sunday': 3,\n",
              "         'Blue': 1,\n",
              "         'running': 1,\n",
              "         'America': 1,\n",
              "         'voters': 1,\n",
              "         'baseball': 1,\n",
              "         'much': 3,\n",
              "         '?': 5,\n",
              "         'Nations': 1,\n",
              "         'needed': 2,\n",
              "         '1959': 1,\n",
              "         'Kowalski': 1,\n",
              "         'interest': 2,\n",
              "         'continued': 1,\n",
              "         'governor': 1,\n",
              "         'next': 2,\n",
              "         'either': 1,\n",
              "         'growth': 1,\n",
              "         'medical': 1,\n",
              "         'gave': 1,\n",
              "         'age': 1,\n",
              "         'am': 1,\n",
              "         'It': 6,\n",
              "         'Providence': 1,\n",
              "         'clear': 1,\n",
              "         'players': 1,\n",
              "         'George': 1,\n",
              "         'area': 2,\n",
              "         'Dallas': 3,\n",
              "         'special': 2,\n",
              "         'only': 5,\n",
              "         'Family': 1,\n",
              "         'Federal': 1,\n",
              "         'even': 3,\n",
              "         'firm': 2,\n",
              "         'followed': 1,\n",
              "         'almost': 1,\n",
              "         'personnel': 1,\n",
              "         'Washington': 3,\n",
              "         'Miami': 1,\n",
              "         'state': 5,\n",
              "         'countries': 2,\n",
              "         'Henry': 1,\n",
              "         'p.m.': 2,\n",
              "         'young': 2,\n",
              "         'higher': 2,\n",
              "         'won': 1,\n",
              "         '1958': 1,\n",
              "         'base': 1,\n",
              "         'official': 1,\n",
              "         'El': 1,\n",
              "         'Democratic': 2,\n",
              "         '``': 24,\n",
              "         'At': 2,\n",
              "         'toward': 1,\n",
              "         'us': 1,\n",
              "         'been': 9,\n",
              "         'Her': 1,\n",
              "         'important': 1,\n",
              "         'nor': 1,\n",
              "         'As': 2,\n",
              "         'apparently': 1,\n",
              "         'start': 2,\n",
              "         \"President's\": 1,\n",
              "         'individual': 1,\n",
              "         'use': 2,\n",
              "         'army': 1,\n",
              "         'administration': 3,\n",
              "         'concerned': 1,\n",
              "         'small': 2,\n",
              "         'time': 5,\n",
              "         'score': 1,\n",
              "         'hear': 1,\n",
              "         'An': 1,\n",
              "         'felt': 1,\n",
              "         'another': 2,\n",
              "         'weeks': 1,\n",
              "         'across': 1,\n",
              "         'days': 2,\n",
              "         'Union': 1,\n",
              "         'because': 3,\n",
              "         'hardly': 1,\n",
              "         'runs': 2,\n",
              "         'efforts': 1,\n",
              "         'So': 1,\n",
              "         \"'\": 3,\n",
              "         'cases': 1,\n",
              "         'address': 1,\n",
              "         'Americans': 1,\n",
              "         'situation': 1,\n",
              "         'Chicago': 1,\n",
              "         'example': 1,\n",
              "         'taking': 1,\n",
              "         'take': 3,\n",
              "         'failed': 1,\n",
              "         'Of': 1,\n",
              "         'leaders': 1,\n",
              "         'That': 2,\n",
              "         'York': 3,\n",
              "         'test': 1,\n",
              "         'involved': 1,\n",
              "         'But': 5,\n",
              "         'Georgia': 1,\n",
              "         'attorney': 1,\n",
              "         'hole': 1,\n",
              "         'welfare': 1,\n",
              "         'loss': 1,\n",
              "         'A': 7,\n",
              "         'U.S.': 3,\n",
              "         'office': 1,\n",
              "         'yesterday': 3,\n",
              "         'between': 3,\n",
              "         'campaign': 1,\n",
              "         'started': 1,\n",
              "         'problems': 1,\n",
              "         'Moritz': 1,\n",
              "         'the': 114,\n",
              "         'taken': 2,\n",
              "         'High': 1,\n",
              "         'whether': 1,\n",
              "         'life': 1,\n",
              "         'First': 1,\n",
              "         'Communist': 2,\n",
              "         'H.': 2,\n",
              "         'more': 8,\n",
              "         'generally': 1,\n",
              "         'movement': 1,\n",
              "         'County': 2,\n",
              "         'million': 3,\n",
              "         'make': 2,\n",
              "         'day': 3,\n",
              "         'police': 2,\n",
              "         'sold': 1,\n",
              "         'white': 1,\n",
              "         'Mantle': 3,\n",
              "         'who': 11,\n",
              "         'In': 6,\n",
              "         'known': 1,\n",
              "         'Island': 1,\n",
              "         'Board': 1,\n",
              "         'around': 2,\n",
              "         'S.': 2,\n",
              "         'policy': 1,\n",
              "         'daily': 1,\n",
              "         'he': 17,\n",
              "         'men': 3,\n",
              "         'April': 1,\n",
              "         'L.': 1,\n",
              "         'State': 3,\n",
              "         'Emory': 1,\n",
              "         'have': 11,\n",
              "         'department': 1,\n",
              "         'be': 19,\n",
              "         'fourth': 1,\n",
              "         'find': 1,\n",
              "         'winning': 1,\n",
              "         'clearly': 1,\n",
              "         'general': 2,\n",
              "         'M.': 1,\n",
              "         'league': 1,\n",
              "         'total': 2,\n",
              "         'women': 1,\n",
              "         'service': 2,\n",
              "         'Army': 1,\n",
              "         'hours': 1,\n",
              "         'executive': 1,\n",
              "         'issue': 2,\n",
              "         'appeared': 1,\n",
              "         \"''\": 24,\n",
              "         'She': 2,\n",
              "         'past': 2,\n",
              "         'Jack': 1,\n",
              "         'Mary': 1,\n",
              "         'States': 2,\n",
              "         'previous': 1,\n",
              "         'Davis': 1,\n",
              "         'library': 2,\n",
              "         'got': 3,\n",
              "         'charge': 1,\n",
              "         'There': 3,\n",
              "         'tax': 3,\n",
              "         'addition': 1,\n",
              "         'group': 2,\n",
              "         'Barnett': 1,\n",
              "         'contract': 1,\n",
              "         'soon': 1,\n",
              "         'John': 4,\n",
              "         'lost': 1,\n",
              "         'candidate': 1,\n",
              "         'football': 1,\n",
              "         'University': 2,\n",
              "         'struck': 1,\n",
              "         'announced': 2,\n",
              "         'reached': 1,\n",
              "         'hospital': 1,\n",
              "         'fall': 1,\n",
              "         'average': 1,\n",
              "         'such': 4,\n",
              "         'building': 1,\n",
              "         'over': 6,\n",
              "         'May': 2,\n",
              "         'came': 2,\n",
              "         'field': 1,\n",
              "         'pitching': 1,\n",
              "         'construction': 1,\n",
              "         'husband': 1,\n",
              "         'big': 2,\n",
              "         'strong': 1,\n",
              "         'get': 4,\n",
              "         'When': 2,\n",
              "         'hand': 1,\n",
              "         'One': 2,\n",
              "         'earlier': 1,\n",
              "         'Yankees': 1,\n",
              "         '12': 1,\n",
              "         'passing': 1,\n",
              "         'also': 6,\n",
              "         'in': 50,\n",
              "         'agreed': 1,\n",
              "         'help': 2,\n",
              "         'an': 12,\n",
              "         'single': 1,\n",
              "         'agreement': 1,\n",
              "         'news': 1,\n",
              "         'long': 2,\n",
              "         'throughout': 1,\n",
              "         'political': 2,\n",
              "         'owners': 1,\n",
              "         'decision': 1,\n",
              "         'both': 3,\n",
              "         'shares': 1,\n",
              "         'perhaps': 1,\n",
              "         ';': 13,\n",
              "         '6': 1,\n",
              "         'Force': 1,\n",
              "         'head': 1,\n",
              "         'it': 14,\n",
              "         'feet': 1,\n",
              "         \"year's\": 1,\n",
              "         'bill': 3,\n",
              "         'seems': 1,\n",
              "         'one': 8,\n",
              "         'Lee': 1,\n",
              "         'work': 4,\n",
              "         'himself': 1,\n",
              "         'hits': 1,\n",
              "         'five': 2,\n",
              "         'high': 3,\n",
              "         'Lawrence': 1,\n",
              "         'revenues': 1,\n",
              "         'vice': 1,\n",
              "         'Kennedy': 4,\n",
              "         'today': 3,\n",
              "         'Angeles': 1,\n",
              "         'walk': 1,\n",
              "         'certain': 1,\n",
              "         '19': 1,\n",
              "         'from': 14,\n",
              "         'became': 1,\n",
              "         'Palmer': 2,\n",
              "         'action': 1,\n",
              "         'weekend': 1,\n",
              "         'prices': 1,\n",
              "         'golf': 1,\n",
              "         'again': 2,\n",
              "         'defense': 1,\n",
              "         'reason': 1,\n",
              "         'Morton': 1,\n",
              "         'that': 26,\n",
              "         'live': 1,\n",
              "         'attack': 1,\n",
              "         'churches': 1,\n",
              "         'meeting': 3,\n",
              "         'keep': 1,\n",
              "         'Government': 1,\n",
              "         'books': 1,\n",
              "         'future': 1,\n",
              "         'basic': 1,\n",
              "         '21': 1,\n",
              "         'companies': 1,\n",
              "         'groups': 1,\n",
              "         'though': 1,\n",
              "         'Monday': 3,\n",
              "         'verdict': 1,\n",
              "         'March': 2,\n",
              "         'stage': 1,\n",
              "         'straight': 1,\n",
              "         'Sam': 1,\n",
              "         'despite': 1,\n",
              "         'book': 1,\n",
              "         'military': 2,\n",
              "         'expected': 2,\n",
              "         'four': 4,\n",
              "         'fund': 1,\n",
              "         'Havana': 1,\n",
              "         'enough': 2,\n",
              "         'No': 1,\n",
              "         'can': 5,\n",
              "         'fact': 1,\n",
              "         'for': 30,\n",
              "         'him': 5,\n",
              "         'current': 1,\n",
              "         'De': 1,\n",
              "         'business': 2,\n",
              "         'About': 1,\n",
              "         'upon': 1,\n",
              "         'bring': 1,\n",
              "         'does': 2,\n",
              "         'provide': 1,\n",
              "         'lives': 1,\n",
              "         'themselves': 1,\n",
              "         'their': 10,\n",
              "         '1961': 2,\n",
              "         'may': 4,\n",
              "         'beat': 1,\n",
              "         'great': 2,\n",
              "         'each': 3,\n",
              "         'Philadelphia': 1,\n",
              "         'schools': 2,\n",
              "         'council': 1,\n",
              "         'student': 1,\n",
              "         'machinery': 1,\n",
              "         'free': 1,\n",
              "         '13': 1,\n",
              "         'others': 1,\n",
              "         'side': 1,\n",
              "         'order': 1,\n",
              "         'information': 1,\n",
              "         'could': 5,\n",
              "         'religious': 1,\n",
              "         'seven': 1,\n",
              "         'Kansas': 1,\n",
              "         'performance': 1,\n",
              "         'there': 6,\n",
              "         'states': 1,\n",
              "         'through': 3,\n",
              "         'will': 15,\n",
              "         'Boston': 1,\n",
              "         'late': 2,\n",
              "         'began': 2,\n",
              "         'Council': 1,\n",
              "         'trade': 1,\n",
              "         'set': 3,\n",
              "         'real': 1,\n",
              "         'charged': 1,\n",
              "         'give': 2,\n",
              "         'do': 3,\n",
              "         'hope': 1,\n",
              "         'study': 1,\n",
              "         'coal': 1,\n",
              "         'lines': 1,\n",
              "         'music': 1,\n",
              "         'admitted': 1,\n",
              "         'open': 2,\n",
              "         'concert': 1,\n",
              "         'personal': 1,\n",
              "         'end': 2,\n",
              "         'thing': 1,\n",
              "         'Democrats': 1,\n",
              "         'money': 2,\n",
              "         'sent': 1,\n",
              "         'Rev.': 1,\n",
              "         'included': 1,\n",
              "         'While': 1,\n",
              "         'declared': 1,\n",
              "         'immediately': 1,\n",
              "         'played': 1,\n",
              "         'sales': 3,\n",
              "         'friends': 1,\n",
              "         'Denver': 1,\n",
              "         'people': 3,\n",
              "         'rule': 1,\n",
              "         '.': 89,\n",
              "         'designed': 1,\n",
              "         'His': 2,\n",
              "         'couple': 1,\n",
              "         'jury': 3,\n",
              "         'complete': 1,\n",
              "         'team': 2,\n",
              "         'Premier': 1,\n",
              "         'place': 1,\n",
              "         'favor': 1,\n",
              "         'Robinson': 1,\n",
              "         'To': 2,\n",
              "         'later': 2,\n",
              "         'any': 5,\n",
              "         '17': 1,\n",
              "         'hour': 1,\n",
              "         '14': 1,\n",
              "         'need': 2,\n",
              "         'B.': 2,\n",
              "         'decided': 1,\n",
              "         'The': 26,\n",
              "         'line': 1,\n",
              "         'Catholic': 1,\n",
              "         'history': 1,\n",
              "         'since': 3,\n",
              "         'act': 1,\n",
              "         'a': 52,\n",
              "         'tournament': 1,\n",
              "         'ago': 2,\n",
              "         'Day': 1,\n",
              "         'read': 1,\n",
              "         'Congolese': 1,\n",
              "         'few': 2,\n",
              "         'manager': 1,\n",
              "         'build': 1,\n",
              "         'until': 2,\n",
              "         'federal': 2,\n",
              "         'just': 2,\n",
              "         'here': 4,\n",
              "         'company': 2,\n",
              "         'F.': 1,\n",
              "         'know': 2,\n",
              "         'within': 1,\n",
              "         'during': 3,\n",
              "         'did': 3,\n",
              "         'veteran': 1,\n",
              "         'night': 4,\n",
              "         'than': 7,\n",
              "         'fire': 1,\n",
              "         'Park': 1,\n",
              "         'session': 2,\n",
              "         'economic': 1,\n",
              "         'plan': 3,\n",
              "         'bonds': 2,\n",
              "         'income': 1,\n",
              "         'course': 1,\n",
              "         'tell': 1,\n",
              "         'Legislature': 1,\n",
              "         'where': 3,\n",
              "         'United': 3,\n",
              "         'proposal': 1,\n",
              "         'Jr.': 3,\n",
              "         'but': 8,\n",
              "         'costs': 1,\n",
              "         'center': 2,\n",
              "         'new': 7,\n",
              "         'public': 3,\n",
              "         'merely': 1,\n",
              "         ...})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(unigram_table)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Negative Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
        "    return torch.LongTensor(idxs)\n",
        "\n",
        "def negative_sampling(targets, unigram_table, k):\n",
        "    batch_size = targets.size(0)\n",
        "    neg_samples = []\n",
        "    for i in range(batch_size):\n",
        "        nsample = []\n",
        "        target_index = targets[i].item()\n",
        "        while len(nsample) < k: # num of sampling\n",
        "            neg = random.choice(unigram_table)\n",
        "            if word2index[neg] == target_index:\n",
        "                continue\n",
        "            nsample.append(neg)\n",
        "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
        "    return torch.cat(neg_samples)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_batch  = torch.Tensor(input_batch)\n",
        "target_batch = torch.LongTensor(target_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1374.],\n",
              "        [12330.]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[11938, 12330, 11938],\n",
              "        [12781,  7904, 11920]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_neg = 3\n",
        "negative_sampling(target_batch, unigram_table, num_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([9720])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_batch[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model\n",
        "\n",
        "$$\\mathbf{J}_{\\text{neg-sample}}(\\mathbf{v}_c,o,\\mathbf{U})=-\\log(\\sigma(\\mathbf{u}_o^T\\mathbf{v}_c))-\\sum_{k=1}^K\\log(\\sigma(-\\mathbf{u}_k^T\\mathbf{v}_c))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SkipgramNegSampling(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, emb_size, word2index):\n",
        "        super(SkipgramNegSampling, self).__init__()\n",
        "        self.embedding_v = nn.Embedding(vocab_size, emb_size) # center embedding\n",
        "        self.embedding_u = nn.Embedding(vocab_size, emb_size) # out embedding\n",
        "        self.logsigmoid = nn.LogSigmoid()\n",
        "        self.word2index  = word2index\n",
        "                    \n",
        "    def forward(self, center_words, target_words, negative_words):\n",
        "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
        "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
        "        neg_embeds    = -self.embedding_u(negative_words) # [batch_size, num_neg, emb_size]\n",
        "        \n",
        "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
        "        \n",
        "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
        "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, k, 1]\n",
        "        \n",
        "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1)\n",
        "                \n",
        "        return -torch.mean(loss)\n",
        "    \n",
        "    def get_embed(self, word):\n",
        "        word2index = self.word2index\n",
        "        \n",
        "        try:\n",
        "            index = word2index[word]\n",
        "        except:\n",
        "            index = word2index['<UNK>']\n",
        "            \n",
        "        word = torch.LongTensor([index])\n",
        "        \n",
        "        embed_c = self.embedding_v(word)\n",
        "        embed_o = self.embedding_u(word)\n",
        "        embed   = (embed_c + embed_o) / 2\n",
        "        \n",
        "        return embed[0][0].item(), embed[0][1].item()\n",
        "    \n",
        "    def prediction(self, inputs):\n",
        "        embeds = self.embedding_v(inputs)\n",
        "        \n",
        "        return embeds\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size     = 2 # mini-batch size\n",
        "embedding_size = 2 #so we can later plot\n",
        "model          = SkipgramNegSampling(voc_size, embedding_size, word2index)\n",
        "num_neg        = 10 # num of negative sampling\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | cost: 11.178477 | time: 0m 4s\n",
            "Epoch: 20 | cost: 7.138061 | time: 0m 9s\n",
            "Epoch: 30 | cost: 6.641814 | time: 0m 14s\n",
            "Epoch: 40 | cost: 9.124754 | time: 0m 18s\n",
            "Epoch: 50 | cost: 6.933053 | time: 0m 22s\n",
            "Epoch: 60 | cost: 8.738845 | time: 0m 27s\n",
            "Epoch: 70 | cost: 13.658433 | time: 0m 31s\n",
            "Epoch: 80 | cost: 8.373291 | time: 0m 35s\n",
            "Epoch: 90 | cost: 9.760756 | time: 0m 40s\n",
            "Epoch: 100 | cost: 12.221634 | time: 0m 44s\n",
            "\n",
            "Complete: \n",
            "Total Loss: 12.22 | Time Taken: 0 minutes and 44 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Training\n",
        "num_epochs = 100\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    input_batch, target_batch = random_batch(batch_size, corpus)\n",
        "    \n",
        "    #input_batch: [batch_size, 1]\n",
        "    input_batch = torch.LongTensor(input_batch)\n",
        "    \n",
        "    #target_batch: [batch_size, 1]\n",
        "    target_batch = torch.LongTensor(target_batch)\n",
        "    \n",
        "    #negs_batch:   [batch_size, num_neg]\n",
        "    negs_batch = negative_sampling(target_batch, unigram_table, num_neg)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "        \n",
        "    loss = model(input_batch, target_batch, negs_batch)\n",
        "    \n",
        "    end = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
        "\n",
        "print(f\"\\nComplete: \\nTotal Loss: {loss:2.2f} | Time Taken: {epoch_mins} minutes and {epoch_secs} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def open_file(path_to_file):\n",
        "    content = []  # Initialize content to an empty list to avoid returning None\n",
        "    try:\n",
        "        with open(path_to_file, 'r') as file:\n",
        "            content = file.readlines()  # Read all lines of the file into a list\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file {path_to_file} does not exist.\")  # File not found error\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")  # Handle any other exceptions (e.g., permission issues)\n",
        "\n",
        "    return content  # Return content even if it's empty, but not None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"file/word-test.v1.1.txt\"\n",
        "\n",
        "content = open_file(file_path)\n",
        "\n",
        "semantic = []\n",
        "syntatic = []\n",
        "\n",
        "current_test = semantic\n",
        "for sent in content:\n",
        "    if sent[0] == ':':\n",
        "        current_test = syntatic\n",
        "        continue\n",
        "    \n",
        "    current_test.append(sent.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_space = []\n",
        "\n",
        "for word in vocab:\n",
        "    vector_space.append(model.get_embed(word))\n",
        "\n",
        "vector_space = np.array(vector_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "#scipy version\n",
        "from scipy import spatial\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    norm_a = a / np.linalg.norm(a)  # Normalize vector a\n",
        "    norm_b = b / np.linalg.norm(b)  # Normalize vector b\n",
        "    return 1 - spatial.distance.cosine(norm_a, norm_b)  # Cosine similarity after normalization\n",
        "\n",
        "\n",
        "def cos_sim_scores(vector_space, target_vector):\n",
        "    scores = []\n",
        "    for each_vect in vector_space:\n",
        "        scores.append(cos_sim(target_vector, each_vect))\n",
        "\n",
        "    return np.array(scores)\n",
        "\n",
        "def similarity(model, test_data):\n",
        "    words = test_data.split(\" \")\n",
        "    embeddings = [np.array(model.get_embed(word)) for word in words[:3]]  # Precompute embeddings for all words\n",
        "    embed0, embed1, embed2 = embeddings  # Unpack embeddings\n",
        "    similar_vector = embed1 - embed0 + embed2  # Perform vector arithmetic\n",
        "\n",
        "    similarity_scores = cos_sim_scores(vector_space, similar_vector)\n",
        "    max_score_idx = np.argmax(similarity_scores)\n",
        "    similar_word = index2word[max_score_idx]\n",
        "\n",
        "    return similar_word == words[3]  # Directly return the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Semantic accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic accuracy: 0.00\n"
          ]
        }
      ],
      "source": [
        "sem_total = len(semantic)\n",
        "sem_correct = 0\n",
        "for sent in semantic:\n",
        "    if similarity(model, sent):\n",
        "        sem_correct += 1\n",
        "\n",
        "sem_accuracy = sem_correct / sem_total\n",
        "print(f\"Semantic accuracy: {sem_accuracy:2.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Syntactic Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Syntatic accuracy: 0.00\n"
          ]
        }
      ],
      "source": [
        "syn_total = len(syntatic)\n",
        "syn_correct = 0\n",
        "for sent in syntatic:\n",
        "    if similarity(model, sent):\n",
        "        syn_correct += 1\n",
        "\n",
        "syn_accuracy = syn_correct / syn_total\n",
        "print(f\"Syntatic accuracy: {syn_accuracy:2.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarity Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"file/wordsim_similarity_goldstandard.txt\"\n",
        "\n",
        "content = open_file(file_path)\n",
        "\n",
        "sim_data = []\n",
        "\n",
        "for sent in content:\n",
        "    sim_data.append(sent.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_similarity(model, test_data):\n",
        "    words = test_data.split(\"\\t\")\n",
        "\n",
        "    embed0 = np.array(model.get_embed(words[0].strip()))\n",
        "    embed1 = np.array(model.get_embed(words[1].strip()))\n",
        "\n",
        "    similarity_model = embed1 @ embed0.T\n",
        "    similarity_provided = float(words[2].strip())\n",
        "\n",
        "    return similarity_provided, similarity_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_scores = []\n",
        "model_scores = []\n",
        "for sent in sim_data:\n",
        "    ds_score, model_score = compute_similarity(model, sent)\n",
        "\n",
        "    ds_scores.append(ds_score)\n",
        "    model_scores.append(model_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation between the dataset similarity metrics and models dot product is 0.04.\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "correlation = spearmanr(ds_scores, model_scores)[0]\n",
        "\n",
        "print(f\"Correlation between the dataset similarity metrics and models dot product is {correlation:2.2f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and arguments saved to model\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Define the folder where want to save the files\n",
        "model_folder = 'model'  # Change this to your desired folder path\n",
        "\n",
        "# Save the model's state_dict\n",
        "torch.save(model.state_dict(), f'{model_folder}/neg.model')\n",
        "\n",
        "# Save the arguments (such as voc_size, emb_size, word2index)\n",
        "neg_args = {\n",
        "    'vocab_size': voc_size,\n",
        "    'emb_size': embedding_size,\n",
        "    'word2index': word2index,\n",
        "}\n",
        "with open(f'{model_folder}/neg.args', 'wb') as f:\n",
        "    pickle.dump(neg_args, f)\n",
        "\n",
        "print(f\"Model and arguments saved to {model_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ekkar\\AppData\\Local\\Temp\\ipykernel_9308\\3154215120.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_neg.load_state_dict(torch.load(f'{model_folder}/neg.model'))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Define the folder where the files are saved\n",
        "model_folder = 'model'  # Change this to the folder where you saved the files\n",
        "\n",
        "# Load the arguments from the pickle file\n",
        "with open(f'{model_folder}/neg.args', 'rb') as f:\n",
        "    neg_args = pickle.load(f)\n",
        "\n",
        "# Define the model class and initialize it with the loaded arguments\n",
        "# Make sure the model class and arguments match the training code\n",
        "model_neg = SkipgramNegSampling(**neg_args)  # Assuming you have a neg model class\n",
        "\n",
        "# Now, load the model weights (this should be from neg.model, not neg.args)\n",
        "model_neg.load_state_dict(torch.load(f'{model_folder}/neg.model'))\n",
        "\n",
        "# Now the model is loaded with the arguments and weights, and you're ready to use it\n",
        "model_neg.eval()  # Set the model to evaluation mode if you're not training\n",
        "\n",
        "print(\"Model loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.5001720786094666, 0.24827289581298828)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_neg.get_embed('the')"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
